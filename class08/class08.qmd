---
title: "Class 8: Breast Cancer Mini Project"
author: "Jason (A15796973)"
format: pdf
---

## Outline
Today we will apply the machine learning methods we introduced in the last class on breast cancer biopsy data from the fine needle aspiration (FNA).

# Data input
The data is supplied on CSV format:

```{r}
wisc.df = read.csv("WisconsinCancer.csv", row.names= 1 )
head(wisc.df)
```

Removing diagnosis column
```{r}
wisc.data <- wisc.df[,-1]
wisc.data
```
Storing diagnosis column
```{r}
diagnosis = as.factor(wisc.df$diagnosis)
diagnosis
```

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.data)
```


Q2. How many of the observations have a malignant diagnosis?
```{r}
table(diagnosis)
```


Q3. How many variables/features in the data are suffixed with _mean?
```{r}
length( grep("mean", colnames(wisc.data)) )
```

Determining if re-scaling is needed
```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

PCA and Summary
```{r}
wisc.pr <- prcomp( wisc.data, scale=TRUE)
summary(wisc.pr)
```
## Interperting Results

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
```{r}
biplot(wisc.pr)
```
The plot looks like a jumbled mess and is difficult to understand because there are way too many points clustered together with a bunch of names.


Better plot
```{r}
plot( wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis )
```


Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?


0.4427



Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?


3


Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?


7



Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[, 1], wisc.pr$x[,3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```

I noticed that both this and the previous plot have a bit of the PC1 observations overlapping with the other PC observations, but it overlaps more with PC3 than it does with PC2.

### Better plot to help understand
```{r}
df = as.data.frame(wisc.pr$x)
df$diagnosis = diagnosis
library(ggplot2)

ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
## Variance

```{r}
pr.var = wisc.pr$sdev^2
head(pr.var)
```
Finding and plotting the amount of variance explained by every component
```{r}
pve = pr.var / sum(pr.var)

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Fun alternative graph
```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean",1]
```


Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
sum( pve[1:5] )

print("or solve by")

x = 0
x= x + 1
x
sum( pve [1:x] )
x= x + 1
x
sum( pve [1:x] )
x= x + 1
x
sum( pve [1:x] )
x= x + 1
x
sum( pve [1:x] )
x= x + 1
x
sum( pve [1:x] )
## It would take 5 components minimum to explain 80% of the variance in data
```
## Do section 5 as last section, up to question 15.

## Hierarchical clustering

```{r}
data.scaled = scale(wisc.data)
data.dist = dist(data.scaled)
wisc.hclust = hclust(data.dist, "complete")
```

Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```
Height 19

```{r}
wisc.hclust.clusters = cutree(wisc.hclust, 4)
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clustersrepeat = cutree(wisc.hclust, 10)
table(wisc.hclust.clustersrepeat, diagnosis)
```

Increasing the clusters allows for the finding of more matches of both benign and malignant

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

My favorite is complete because from my viewpoint it is the most "comprehensive" by going for the largest similarities.

## K-means Clustering

```{r}
wisc.km = kmeans(scale(wisc.data), centers= 2, nstart= 20)
```

```{r}
table(wisc.km$cluster, diagnosis)
```

Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

From my viewpoint, this seems to separate the two diagnoses just as well as the hclust. It may actually be better since the other two extra clusters when doing the hclust was not that useful.

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

## Combining methods

```{r}
wisc.pr.hclust = hclust(data.dist, method="ward.D2")

grps = cutree(wisc.pr.hclust, k=2)
table(grps)

table(grps, diagnosis)

plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)

```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```

Q15. How well does the newly created model with four clusters separate out the two diagnoses?

The new model is performs its job just as well as the previous kmeans method.